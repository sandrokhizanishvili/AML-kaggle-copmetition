{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ad37f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "834df683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    data = dict(np.load(path, allow_pickle=True))\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def prepare_train_data(data):\n",
    "    \"\"\"Prepare training data from loaded dict\"\"\"\n",
    "    caption_embd = data['captions/embeddings']\n",
    "    image_embd = data['images/embeddings']\n",
    "    # Map caption embeddings to corresponding image embeddings\n",
    "    label = data['captions/label'] # N x M\n",
    "\n",
    "    # repeat the image embeddings according to the label\n",
    "    label_idx = np.nonzero(label)[1]\n",
    "    print(label_idx.shape)\n",
    "    image_embd = image_embd[label_idx]\n",
    "    assert caption_embd.shape[0] == image_embd.shape[0], \"Mismatch in number of caption and image embeddings\"\n",
    "\n",
    "    X = torch.from_numpy(caption_embd).float()\n",
    "    # Map each caption to its corresponding image embedding\n",
    "    y = torch.from_numpy(image_embd).float()\n",
    "    label = torch.from_numpy(label).bool()\n",
    "\n",
    "    print(f\"Train data: {len(X)} captions, {len(image_embd)} images\")\n",
    "    return X, y, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50850835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125000,)\n",
      "Train data: 125000 captions, 125000 images\n",
      "Data shapes - X: torch.Size([125000, 1024]), y: torch.Size([125000, 1536]), labels: torch.Size([125000, 25000])\n"
     ]
    }
   ],
   "source": [
    "# train_data = load_data('/content/data/train/train.npz')\n",
    "train_data = load_data('data/train/train/train.npz')\n",
    "X, y, labels = prepare_train_data(train_data)\n",
    "del train_data\n",
    "gc.collect()\n",
    "print(f\"Data shapes - X: {X.shape}, y: {y.shape}, labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808dfae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Preprocessed data:\n",
      "  Train: X=torch.Size([112500, 1024]), y=torch.Size([112500, 1536])\n",
      "  Val:   X=torch.Size([12500, 1024]), y=torch.Size([12500, 1536])\n",
      "  Train: labels=torch.Size([112500])\n",
      "  Val:   labels=torch.Size([12500])\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(X, y, label, train_split=0.9):\n",
    "    \"\"\"Preprocess data: split, standardize, and pad\"\"\"\n",
    "    # Calculate split indices first\n",
    "    n_train = int(train_split * len(X))\n",
    "    perm = torch.randperm(len(X))\n",
    "    train_indices = perm[:n_train]\n",
    "    val_indices = perm[n_train:]\n",
    "    \n",
    "    # Process and delete original data in chunks to reduce memory usage\n",
    "    # Split data first\n",
    "    X_train, X_val = X[train_indices], X[val_indices]\n",
    "    y_train, y_val = y[train_indices], y[val_indices]\n",
    "    \n",
    "    # Free original arrays immediately\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "    \n",
    "    # Normalize in-place where possible to avoid creating temporary copies\n",
    "    sc_x = StandardScaler()\n",
    "    X_train_scaled = torch.from_numpy(sc_x.fit_transform(X_train.numpy())).float()\n",
    "    X_val_scaled = torch.from_numpy(sc_x.transform(X_val.numpy())).float()\n",
    "\n",
    "    sc_y = StandardScaler()\n",
    "    y_train_scaled = torch.from_numpy(sc_y.fit_transform(y_train.numpy())).float()\n",
    "    y_val_scaled = torch.from_numpy(sc_y.transform(y_val.numpy())).float()\n",
    "    \n",
    "    # Free intermediate arrays\n",
    "    # del X_train, X_val, y_train, y_val\n",
    "    # gc.collect()\n",
    "    \n",
    "    # Process labels more efficiently\n",
    "    # Use boolean indexing directly without creating intermediate variables\n",
    "    label_train_subset = label[train_indices]\n",
    "    label_val_subset = label[val_indices]\n",
    "    \n",
    "    # Calculate label indices\n",
    "    img_TRAIN_SPLIT = label_train_subset.sum(dim=0) > 0\n",
    "    labels_train_ind = torch.nonzero(label_train_subset[:, img_TRAIN_SPLIT])[:, 1]\n",
    "    \n",
    "    img_VAL_SPLIT = label_val_subset.sum(dim=0) > 0\n",
    "    labels_val_ind = torch.nonzero(label_val_subset[:, img_VAL_SPLIT])[:, 1]\n",
    "    \n",
    "    # Free label subsets\n",
    "    del label_train_subset, label_val_subset\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\" Preprocessed data:\")\n",
    "    print(f\"  Train: X={X_train_scaled.shape}, y={y_train_scaled.shape}\")\n",
    "    print(f\"  Val:   X={X_val_scaled.shape}, y={y_val_scaled.shape}\")\n",
    "    print(f\"  Train: labels={labels_train_ind.shape}\")\n",
    "    print(f\"  Val:   labels={labels_val_ind.shape}\")\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, X_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled, labels_train_ind, labels_val_ind, sc_x, sc_y\n",
    "\n",
    "X_train, X_val, y_train, y_val, X_train_scaled, X_val_scaled, y_train_scaled, y_val_scaled, labels_train, labels_val, sc_x, sc_y = preprocess_data(X, y, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43a18f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scalers as a pickle file\n",
    "with open('scaler_x_15_11_25.pkl', 'wb') as f:\n",
    "    pickle.dump(sc_x, f)\n",
    "\n",
    "with open('scaler_y_15_11_25.pkl', 'wb') as f:\n",
    "    pickle.dump(sc_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "644626b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data tensors\n",
    "torch.save({\n",
    "    'X_train': X_train,\n",
    "    'X_val': X_val,\n",
    "    'y_train': y_train,\n",
    "    'y_val': y_val,\n",
    "    'X_train_scaled': X_train_scaled,\n",
    "    'X_val_scaled': X_val_scaled,\n",
    "    'y_train_scaled': y_train_scaled,\n",
    "    'y_val_scaled': y_val_scaled,\n",
    "    'labels_train': labels_train,\n",
    "    'labels_val': labels_val\n",
    "}, 'data/sandros_data_15_11_25.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1059d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data back\n",
    "data = torch.load('data/sandros_data_15_11_25.pt')\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_train_scaled = data['X_train_scaled']\n",
    "X_val = data['X_val']\n",
    "X_val_scaled = data['X_val_scaled']\n",
    "\n",
    "y_train = data['y_train']\n",
    "y_train_scaled = data['y_train_scaled']\n",
    "y_val = data['y_val']\n",
    "y_val_scaled = data['y_val_scaled']\n",
    "\n",
    "labels_train = data['labels_train']\n",
    "labels_val = data['labels_val']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
